{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12227af5-05a2-42a8-bc64-7965bf751038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Callable\n",
    "import SimpleITK as sitk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "from monai import transforms as mt\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data import CacheDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c5a79f-e5b8-4d01-8975-e6f85a7afb51",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415b4d59-f283-44a0-8b1f-19b22ee307db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    image: torch.Tensor,\n",
    "    label: torch.Tensor | None = None,\n",
    "    crop_size: tuple[int, ...] = (28, 28, 28)\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    # Normalize the image (Z-score normalization)\n",
    "    image = (image - image.mean() ) / image.std()\n",
    "    \n",
    "    # Add a channel dimension to the image\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Random crop\n",
    "    crop_origin = [0, 0, 0]\n",
    "    for dim in range(3):  # Remember, image.shape = [Ch, X, Y, Z ]\n",
    "        max_value = image.shape[dim+1] - crop_size[dim]\n",
    "        crop_origin[dim] = torch.randint(0, max_value, (1,)).item()\n",
    "        \n",
    "    image = image[\n",
    "        :,\n",
    "        crop_origin[0]:crop_origin[0] + crop_size[0],\n",
    "        crop_origin[1]:crop_origin[1] + crop_size[1],\n",
    "        crop_origin[2]:crop_origin[2] + crop_size[2],\n",
    "    ]\n",
    "    \n",
    "    # Add a channel dimension to the label\n",
    "    if label is not None:\n",
    "        label = label.unsqueeze(0)\n",
    "\n",
    "        label = label[\n",
    "            :,\n",
    "            crop_origin[0]:crop_origin[0] + crop_size[0],\n",
    "            crop_origin[1]:crop_origin[1] + crop_size[1],\n",
    "            crop_origin[2]:crop_origin[2] + crop_size[2],\n",
    "        ]\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c96e35ad-99aa-4080-8138-59d8e30b0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_samples(root: Path, is_test: bool = False) -> list[dict[str, Path]]:\n",
    "    \"\"\"\n",
    "    Collects the samples from the WMH dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : Path\n",
    "        The root directory of the dataset.\n",
    "    is_test : bool\n",
    "        Whether to collect the test set or the training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Path]]\n",
    "        A list of dictionaries containing the image and label paths.\n",
    "    \"\"\"\n",
    "  \n",
    "    samples = []\n",
    "    # Iterate through each subject directory\n",
    "    for subject_dir in root.iterdir():\n",
    "        if subject_dir.is_dir():  # Check if it's a directory\n",
    "            flair_path = subject_dir / \"FLAIR.nii.gz\"\n",
    "            t1_path = subject_dir / \"T1.nii.gz\"\n",
    "            WMH_path = subject_dir / \"wmh.nii.gz\"\n",
    "            \n",
    "            # Check if all required files exist\n",
    "            if flair_path.exists() and t1_path.exists() and WMH_path.exists():\n",
    "                sample = {\n",
    "                    \"flair\": flair_path,\n",
    "                    \"t1\": t1_path,\n",
    "                    \"WMH\": WMH_path\n",
    "                }\n",
    "                samples.append(sample)\n",
    "            else:\n",
    "                print(f\"Missing files in {subject_dir}: \"\n",
    "                      f\"{'FLAIR' if not flair_path.exists() else ''} \"\n",
    "                      f\"{'T1' if not t1_path.exists() else ''} \"\n",
    "                      f\"{'wmh' if not WMH_path.exists() else ''}\")\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c06f37-e880-4b21-bf55-ee41d4de7991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x167517490>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/transforms/transform.py:140\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m map_items:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/transforms/transform.py:140\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m map_items:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/transforms/io/dictionary.py:162\u001b[0m, in \u001b[0;36mLoadImaged.__call__\u001b[0;34m(self, data, reader)\u001b[0m\n\u001b[1;32m    161\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(data)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, meta_key, meta_key_postfix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_key_postfix):\n\u001b[1;32m    163\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loader(d[key], reader)\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/transforms/transform.py:475\u001b[0m, in \u001b[0;36mMapTransform.key_iterator\u001b[0;34m(self, data, *extra_iterables)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_missing_keys:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` of transform `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` was missing in the data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and allow_missing_keys==False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    478\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Key `image` of transform `LoadImaged` was missing in the data and allow_missing_keys==False.'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m val_samples \u001b[38;5;241m=\u001b[39m samples[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(samples) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.8\u001b[39m):]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# You might get an error here, make sure you install the required extra dependencies\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mCacheDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m val_ds   \u001b[38;5;241m=\u001b[39m CacheDataset(data\u001b[38;5;241m=\u001b[39mval_samples,   transform\u001b[38;5;241m=\u001b[39m val_transforms)\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/data/dataset.py:809\u001b[0m, in \u001b[0;36mCacheDataset.__init__\u001b[0;34m(self, data, transform, cache_num, cache_rate, num_workers, progress, copy_cache, as_contiguous, hash_as_key, hash_func, runtime_cache)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m ListProxy \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hash_keys: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 809\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/data/dataset.py:836\u001b[0m, in \u001b[0;36mCacheDataset.set_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    833\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_num))\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# prepare cache content immediately\u001b[39;00m\n\u001b[0;32m--> 836\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;66;03m# this must be in the main process, not in dataloader's workers\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/data/dataset.py:865\u001b[0m, in \u001b[0;36mCacheDataset._fill_cache\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress \u001b[38;5;129;01mand\u001b[39;00m has_tqdm:\n\u001b[0;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cache_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(p\u001b[38;5;241m.\u001b[39mimap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cache_item, indices))\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/data/dataset.py:878\u001b[0m, in \u001b[0;36mCacheDataset._load_cache_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    873\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m    875\u001b[0m first_random \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mget_index_of_first(\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28misinstance\u001b[39m(t, RandomizableTrait) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, Transform)\n\u001b[1;32m    877\u001b[0m )\n\u001b[0;32m--> 878\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_random\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_contiguous:\n\u001b[1;32m    881\u001b[0m     item \u001b[38;5;241m=\u001b[39m convert_to_contiguous(item, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/transforms/compose.py:335\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m     _lazy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[0;32m--> 335\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name\u001b[38;5;241m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/Education/Bioinformatics and biocomplexity/AI for medical imaging/project/med_Unet/venv/lib/python3.10/site-packages/monai/transforms/transform.py:171\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         _log_stats(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplying transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x167517490>"
     ]
    }
   ],
   "source": [
    "# TODO - What does this do? \n",
    "# is this data augmentation?\n",
    "# why is the transform different for train and validation?\n",
    "\n",
    "train_transforms = mt.Compose([\n",
    "    mt.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    mt.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    mt.NormalizeIntensityd(keys=[\"image\"]),\n",
    "    mt.RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[28, 28, 28]),\n",
    "    # You can add more transforms here! See\n",
    "    # https://docs.monai.io/en/stable/transforms.html#dictionary-transforms\n",
    "])\n",
    "\n",
    "val_transforms = mt.Compose([\n",
    "    mt.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    mt.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    mt.NormalizeIntensityd(keys=[\"image\"]),\n",
    "])\n",
    "\n",
    "\n",
    "# Actually running loading \n",
    "# TODO - Create a data folder and put the wmh data there in accordance with this path\n",
    "root = Path(abspath(\"\"))\n",
    "wmh_path = root.joinpath(\"data/WMH\")\n",
    "sample_paths = [item for item in wmh_path.iterdir() if item.is_dir()]\n",
    "samples = []\n",
    "\n",
    "for sample_path in sample_paths:\n",
    "    samples.append(collect_samples(sample_path))\n",
    "\n",
    "# Train and test split\n",
    "# TODO - Implement shuffeling of the dataset \n",
    "# TODO - also make a test dataset\n",
    "# Maybe think about balancing the dataset aswell between singapore, amsterdam and utrecht. \n",
    "# if metadata about patient health status, gender ect is availible we should maybe also balance based on that\n",
    "train_samples = samples[:int(len(samples) * 0.8)]\n",
    "val_samples = samples[int(len(samples) * 0.8):]\n",
    "\n",
    "# You might get an error here, make sure you install the required extra dependencies\n",
    "# https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
    "train_ds = CacheDataset(data=train_samples, transform= train_transforms)\n",
    "val_ds   = CacheDataset(data=val_samples,   transform= val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7d2556-1e1b-42a7-af7b-9726b48ee43d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# root = Path(r\"./WMH/WMH/Amsterdam\").resolve()\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# root2 = Path(r\"./WMH/WMH/Singapore\").resolve()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Visualize the first sample if available\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m samples:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mvisualize_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Visualize the first sample\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo samples found to visualize.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36mvisualize_sample\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_sample\u001b[39m(sample):\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Visualize FLAIR, T1, and WMH images from a sample.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     flair_image \u001b[38;5;241m=\u001b[39m load_nifti_image(\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflair\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      9\u001b[0m     t1_image \u001b[38;5;241m=\u001b[39m load_nifti_image(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m     label_image \u001b[38;5;241m=\u001b[39m load_nifti_image(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWMH\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "def load_nifti_image(image_path: Path):\n",
    "    \"\"\"Load a NIfTI image using SimpleITK.\"\"\"\n",
    "    image = sitk.ReadImage(str(image_path))\n",
    "    return sitk.GetArrayFromImage(image)  # Convert to NumPy array\n",
    "\n",
    "def visualize_sample(sample):\n",
    "    \"\"\"Visualize FLAIR, T1, and WMH images from a sample.\"\"\"\n",
    "    flair_image = load_nifti_image(sample['flair'])\n",
    "    t1_image = load_nifti_image(sample['t1'])\n",
    "    label_image = load_nifti_image(sample['WMH'])\n",
    "\n",
    "    # Select a slice to visualize (e.g., the middle slice)\n",
    "    slice_index = flair_image.shape[0] // 2\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # FLAIR Image\n",
    "    axes[0].imshow(flair_image[slice_index, :, :], cmap='gray')\n",
    "    axes[0].set_title('FLAIR Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # T1 Image\n",
    "    axes[1].imshow(t1_image[slice_index, :, :], cmap='gray')\n",
    "    axes[1].set_title('T1 Image')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # WMH Label\n",
    "    axes[2].imshow(label_image[slice_index, :, :], cmap='gray')\n",
    "    axes[2].set_title('WMH')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# root = Path(r\"./WMH/WMH/Amsterdam\").resolve()\n",
    "# root2 = Path(r\"./WMH/WMH/Singapore\").resolve()\n",
    "# root3 = Path(r\"./WMH/WMH/Utrecht\").resolve()\n",
    "# samples = collect_samples(root) + collect_samples(root2) + collect_samples(root3)\n",
    "\n",
    "# Visualize the first sample if available\n",
    "if samples:\n",
    "    visualize_sample(samples[0])  # Visualize the first sample\n",
    "else:\n",
    "    print(\"No samples found to visualize.\")\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c026b2-cca6-41ab-9f90-5404979b77b1",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10efc8-0281-4488-b7eb-66dbcdd854c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class holding the double convolution\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # For now we are just considering 2d input\n",
    "        # Thus expected input dimensions is [batch_size, channels, height, width] \n",
    "        # Or when not using batches [channels, height, width]\n",
    "        # Or in the convention of pytorch: (N, C, H, W) or (C, H, W)\n",
    "\n",
    "        # nn.Identity just return it's input so it's used as a replacement for normalization if normalization is not used\n",
    "        # TO DO: find out what batchnorm does exactly\n",
    "        # TO DO: Find out how exactly relu works\n",
    "        conv = nn.Conv2d\n",
    "        norm = nn.BatchNorm2d if use_norm else nn.Identity \n",
    "        activation_function = nn.ReLU\n",
    "\n",
    "        layers = [\n",
    "            conv(in_channels, out_channels, 3),\n",
    "            norm(out_channels),\n",
    "            activation_function(inplace=True),\n",
    "            conv(out_channels, out_channels, 3),\n",
    "            norm(out_channels),\n",
    "            activation_function(inplace=True)    \n",
    "        ]\n",
    "\n",
    "        self.double_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccadf45-6da2-49e6-a8b0-c2172e525d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class holding the downsampling\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # with kernel_size 2 and stride 2 the dimensions will be halved\n",
    "        self.downsample = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.downsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595824c0-85a8-4107-90a5-97d1c5e478c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class holding the upsampling\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # with kernel_size 2 and stride 2 the dimensions will be doubled\n",
    "        self.upsample = nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b6ada-aab2-440d-9c0b-e35400caad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class holding the entire model:\n",
    "class SegmentUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        depth: int = 3,\n",
    "        first_layer_channel_count: int = 64,\n",
    "        use_norm: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the input layer\n",
    "        layers = [\n",
    "            DoubleConv(\n",
    "                in_channels, \n",
    "                first_layer_channel_count, \n",
    "                use_norm\n",
    "            ),\n",
    "            DownSample(\n",
    "                in_channels=first_layer_channel_count,\n",
    "                out_channels=first_layer_channel_count*2\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        current_channels = first_layer_channel_count*2\n",
    "        # Encoder path\n",
    "        for _ in range(depth-1): # minus one to account for input layer\n",
    "            layers.append(DoubleConv(in_channels=current_channels, out_channels=current_channels, use_norm=use_norm))\n",
    "            layers.append(DownSample(in_channels=current_channels, out_channels=out_channels*2))\n",
    "            current_channels = current_channels*2 # double channel count each encoder block\n",
    "        \n",
    "        # Middle layer\n",
    "        layers.append(DoubleConv(in_channels=current_channels, out_channels=current_channels, use_norm=use_norm))\n",
    "        \n",
    "        # Decoder path\n",
    "        for _ in range(depth-1): # minus one to account for output layer\n",
    "            # Keep in mind here that the double conv layers here gets both the output  \n",
    "            # of upsample concatanated with the skip conncention.\n",
    "            # So number of channels is doubled\n",
    "            # We control this by concatanation in the second dimension \n",
    "            # the convention of pytorch: (N, C, H, W) or (C, H, W)\n",
    "            \n",
    "            layers.append(UpSample(in_channels=current_channels*2, out_channels=current_channels))\n",
    "            layers.append(DoubleConv(in_channels=current_channels, out_channels=current_channels, use_norm=use_norm))\n",
    "            current_channels = current_channels // 2\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(UpSample(in_channels=current_channels*2, out_channels=current_channels))\n",
    "        layers.append(DoubleConv(in_channels=current_channels, out_channels=out_channels, use_norm=use_norm))\n",
    "\n",
    "        # Concatanate layers together\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        pass\n",
    "        #for layer in self.layers[0:depth]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38b77c-6a79-4104-a956-403063edac30",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d1755-bb64-4010-afe4-71ab02efb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTraining():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_epochs: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: float,\n",
    "        loss_fn: Callable,\n",
    "        samples: list\n",
    "    ) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SegmentUNet(in_channels=1, out_channels=1, depth=3).to(self.device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.train_ds, self.train_dl, self.val_ds, self.val_dl = self._split_samples(samples)\n",
    "    \n",
    "    def run_training_loop(self):\n",
    "        for epoch in (prog_bar := tqdm(range(self.n_epochs), desc=\"Training\", unit=\"epoch\", total=self.n_epochs, position=0)):\n",
    "            prog_bar.set_description(f\"Training Loop\")\n",
    "            train_losses = self._loop_train()\n",
    "            prog_bar.set_postfix({\"Training loss\": sum(train_losses) / len(train_losses)})\n",
    "            prog_bar.set_description(f\"Validation Loop\")\n",
    "            val_losses = self._loop_validate()\n",
    "            prog_bar.set_postfix({\"Training loss\": sum(train_losses) / len(train_losses), \"Validation loss\": sum(val_losses) / len(val_losses)})\n",
    "        \n",
    "    def _loop_train(self):\n",
    "        self.model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for i, (image, label) in tqdm(enumerate(self.train_dl), total=len(self.train_dl), desc=\"Training\", unit=\"batch\", position=1, leave=False):\n",
    "            \n",
    "            image, label = image.to(self.device), label.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad() # Clear gradients\n",
    "            output = self.model(...) # Model forward pass\n",
    "            loss = loss_fn(output, label)  # Compute loss\n",
    "            loss.backward()  # Backpropagate loss\n",
    "            self.optimizer.step()  # Update model weights\n",
    "\n",
    "            train_losses.append(loss.item()) # Append training loss for this batch\n",
    "\n",
    "        return train_losses\n",
    "\n",
    "    def _loop_validate(self):\n",
    "        self.model.eval() # We set the model in evaluation mode\n",
    "        val_losses = []\n",
    "        for i, (image, label) in tqdm(enumerate(self.val_dl), total=len(self.val_dl), desc=\"Validation\", unit=\"batch\", position=1, leave=False):\n",
    "            image, label = image.to(self.device), label.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.model(image)\n",
    "                loss = loss_fn(output, label)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "        return val_losses\n",
    "\n",
    "    def _split_samples(self, samples): #TODO - adjust this function to our data\n",
    "        train_samples = samples[0 : int(len(samples) * 0.8)]\n",
    "        val_samples = samples[int(len(samples) * 0.8): ]\n",
    "\n",
    "        train_ds = MedicalDecathlonDataset(train_samples) #TODO - change to our own dataset class\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True) #TODO - change to our DataLoader class\n",
    "\n",
    "        val_ds = MedicalDecathlonDataset(val_samples) #TODO - change to our own dataset class\n",
    "        val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False) #TODO - change to our DataLoader class\n",
    "        return train_ds, train_dl, val_ds, val_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63935882-0dc8-4228-b857-bd3e66bae9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = [nn.BCEWithLogitsLoss()] #TODO - add different loss functions to evaluate\n",
    "for loss_fn in loss_functions:\n",
    "    Model_training = ModelTraining(\n",
    "        n_epochs = 10,\n",
    "        batch_size = 4,\n",
    "        learning_rate = 1e-3,\n",
    "        loss_fn = loss_fn,\n",
    "        samples = []\n",
    "        )\n",
    "\n",
    "    Model_training.run_training_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
